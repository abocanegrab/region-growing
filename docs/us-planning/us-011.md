# US-011: Sistema de AnÃ¡lisis JerÃ¡rquico End-to-End (API REST + CLI)

**Estado:** âœ… COMPLETADA
**Fecha de CreaciÃ³n:** 11 de Noviembre de 2025
**Fecha de FinalizaciÃ³n:** 13 de Noviembre de 2025
**Equipo:** 24 - Region Growing
**Desarrollador Asignado:** Carlos Bocanegra (Backend API), Arthur Zizumbo (Pipeline Core + CLI)
**EstimaciÃ³n:** 8 horas (3h Pipeline Core + 3h API + 2h CLI)
**Prioridad:** CRÃTICA (Entregable final del sistema - IntegraciÃ³n de US-003 a US-010)

---

## ğŸ“‹ Historia de Usuario

**Como** usuario final (agrÃ³nomo, investigador, desarrollador)
**Quiero** un sistema completo end-to-end que orqueste todo el pipeline desde descarga de Sentinel-2 hasta anÃ¡lisis de estrÃ©s clasificado
**Para que** pueda obtener resultados interpretables (objetos clasificados semÃ¡nticamente con anÃ¡lisis de estrÃ©s) mediante:
- **API REST** para integraciÃ³n en aplicaciones web/mÃ³viles (frontend Nuxt 3)
- **CLI script** para uso en terminal, notebooks Jupyter, o automatizaciÃ³n CI/CD

**Dependencias:** US-003 (Sentinel-2), US-006 (Prithvi), US-007 (MGRG), US-010 (ClasificaciÃ³n)

---

## ğŸ¯ Contexto y MotivaciÃ³n

### Problema Actual
- **US-003 a US-010 son componentes individuales**: Requieren ejecuciÃ³n manual paso a paso en notebooks
- **Sin orquestaciÃ³n automÃ¡tica**: Usuario debe conocer el flujo completo y ejecutar cada mÃ³dulo
- **DifÃ­cil integraciÃ³n externa**: No hay interfaz REST para aplicaciones web/mÃ³viles
- **No reproducible fÃ¡cilmente**: Comandos complejos dispersos en mÃºltiples notebooks
- **Sin validaciÃ³n de datos**: No hay verificaciÃ³n automÃ¡tica de disponibilidad de archivos intermedios

### SoluciÃ³n Propuesta
**Sistema Dual: API REST + CLI Script**

#### OpciÃ³n 1: API REST (FastAPI)
- **Para**: Aplicaciones web, dashboards, servicios cloud
- **Ventajas**: AsÃ­ncrono, escalable, documentaciÃ³n automÃ¡tica (Swagger)
- **Uso**: `POST /api/analysis/hierarchical` con bbox y fecha

#### OpciÃ³n 2: CLI Script (Python)
- **Para**: Investigadores, notebooks, automatizaciÃ³n, CI/CD
- **Ventajas**: Simple, rÃ¡pido, no requiere servidor
- **Uso**: `python analyze_region.py --bbox "..." --date "2025-10-15"`

**Ambas opciones comparten la misma lÃ³gica de negocio** (funciones reutilizables).

---

## âœ… Criterios de AceptaciÃ³n

### Funcionalidad Core
- [ ] **API REST Endpoint**: `POST /api/analysis/hierarchical`
  - ParÃ¡metros: bbox, date, threshold_mgrg, export_formats
  - Procesamiento asÃ­ncrono con Celery (opcional)
  - Respuesta: analysis_id, status, download_url

- [ ] **CLI Script**: `scripts/analyze_region.py`
  - Argumentos: `--bbox`, `--date`, `--output`, `--threshold`
  - EjecuciÃ³n sÃ­ncrona con barra de progreso
  - Output: GeoTIFF, JSON, PNG visualizaciones

- [ ] **Pipeline Completo** (6 pasos):
  1. Descargar Sentinel-2 (bandas HLS)
  2. Extraer embeddings Prithvi
  3. Segmentar con MGRG
  4. Clasificar objetos (zero-shot)
  5. Analizar estrÃ©s vegetal (solo cultivos)
  6. Generar reporte (JSON + visualizaciones)

### OrquestaciÃ³n de MÃ³dulos
```python
# Reutilizar componentes existentes:
from src.data.sentinel_downloader import download_sentinel2_hls  # US-003
from src.models.prithvi_wrapper import extract_embeddings        # US-006
from src.algorithms.semantic_region_growing import SemanticRegionGrowing  # US-007
from src.classification.zero_shot_classifier import SemanticClassifier   # US-010
from src.utils.ndvi_calculator import calculate_ndvi             # US-004
```

### Salidas del Sistema
- [ ] **JSON estructurado** con resultados (formato bilingÃ¼e segÃºn US-010):
  ```json
  {
    "metadata": {
      "bbox": [-115.35, 32.45, -115.25, 32.55],
      "date_from": "2025-10-15",
      "date_to": "2025-10-15",
      "zone": "Mexicali",
      "resolution": 10.0,
      "mgrg_threshold": 0.95
    },
    "segmentation": {
      "method": "MGRG",
      "regions": 23,
      "coherence": 99.0,
      "processing_time_s": 3.2
    },
    "classification": [
      {
        "region_id": 5,
        "class": "Vigorous Crop (Cultivo Vigoroso)",
        "class_id": 3,
        "confidence": 0.87,
        "area_ha": 124.5,
        "stress_level": "low",
        "mean_ndvi": 0.72,
        "std_ndvi": 0.05
      }
    ],
    "stress_analysis": {
      "low": {"count": 1, "area_ha": 124.5, "regions": [5]},
      "medium": {"count": 0, "area_ha": 0.0, "regions": []},
      "high": {"count": 0, "area_ha": 0.0, "regions": []}
    },
    "summary": {
      "water_ha": 0.95,
      "bare_soil_ha": 10156.30,
      "stressed_crop_ha": 92.29,
      "vigorous_crop_ha": 0.0,
      "urban_ha": 0.0,
      "grass_shrub_ha": 0.0
    },
    "processing_time": {
      "download": 8.5,
      "embeddings": 12.3,
      "segmentation": 3.2,
      "classification": 0.8,
      "total": 24.8
    }
  }
  ```

- [ ] **GeoTIFF** con capas:
  - Capa 1: SegmentaciÃ³n MGRG (region IDs)
  - Capa 2: ClasificaciÃ³n semÃ¡ntica (class IDs)
  - Capa 3: Mapa de estrÃ©s (stress levels)

- [ ] **Visualizaciones PNG** (300 DPI):
  - Panel 1: RGB original
  - Panel 2: Mapa semÃ¡ntico clasificado
  - Panel 3: Mapa de estrÃ©s (solo cultivos)
  - Panel 4: EstadÃ­sticas (tabla)

### API REST EspecÃ­fico
- [ ] Endpoint documentado en Swagger UI (`/docs`)
- [ ] ValidaciÃ³n de parÃ¡metros con Pydantic (bbox, date, threshold)
- [ ] Manejo de errores con cÃ³digos HTTP apropiados (400, 404, 500, 503)
- [ ] Rate limiting (10 requests/min por IP) - opcional para MVP
- [ ] Timeout de 5 minutos mÃ¡ximo por anÃ¡lisis
- [ ] Background tasks con FastAPI BackgroundTasks (no Celery para MVP)
- [ ] Storage en memoria para status (Redis en producciÃ³n)
- [ ] CORS configurado para frontend Nuxt 3 (localhost:3000)

### CLI Script EspecÃ­fico
- [ ] Argumentos con `argparse` bien documentados
- [ ] Barra de progreso con `tqdm`
- [ ] Logging a archivo y consola
- [ ] Exit codes informativos (0=success, 1=error)
- [ ] Ejemplo de uso en `--help`

### Testing
- [ ] Tests de integraciÃ³n end-to-end (API + CLI)
- [ ] Test con bbox vÃ¡lido Mexicali (debe completar en <40s con datos cacheados)
- [ ] Test con bbox invÃ¡lido (debe fallar gracefully con mensaje claro)
- [ ] Test de timeout (simular Sentinel-2 lento con mock)
- [ ] Test de validaciÃ³n de datos intermedios (NDVI, embeddings, segmentaciÃ³n)
- [ ] Test de formatos de salida (JSON, TIF, PNG)
- [ ] Mock de componentes externos (Sentinel Hub, Prithvi) para tests rÃ¡pidos

### Cumplimiento AGENTS.md
- [ ] CÃ³digo en inglÃ©s (funciones, variables, clases)
- [ ] DocumentaciÃ³n en espaÃ±ol (narrativa, comentarios explicativos)
- [ ] Funciones reutilizables en `src/pipeline/` (compartidas entre API y CLI)
- [ ] No cÃ³digo duplicado entre API y CLI (lÃ³gica en `HierarchicalAnalysisPipeline`)
- [ ] Type hints en todas las funciones pÃºblicas
- [ ] Docstrings estilo Google en todas las clases y mÃ©todos
- [ ] Logging profesional con `logger` (no `print`)
- [ ] Sin emojis en cÃ³digo Python
- [ ] Nombres bilingÃ¼es en outputs (inglÃ©s/espaÃ±ol) segÃºn US-010
- [ ] Tests unitarios >70% cobertura
- [ ] Un solo documento de resoluciÃ³n en `docs/us-resolved/us-011.md` al finalizar

---

## ğŸ—ï¸ Arquitectura de la SoluciÃ³n

### Diagrama de Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SISTEMA DE ANÃLISIS JERÃRQUICO                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OPCIÃ“N 1: API REST      â”‚        â”‚   OPCIÃ“N 2: CLI SCRIPT    â”‚
â”‚   (FastAPI + Celery)      â”‚        â”‚   (Python Standalone)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   CORE PIPELINE LOGIC         â”‚
              â”‚   src/pipeline/hierarchical   â”‚
              â”‚   (Shared by both options)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚                   â”‚
         â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ US-003  â”‚        â”‚ US-006  â”‚        â”‚ US-007  â”‚
    â”‚ Downloadâ”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Embeddingsâ”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  MGRG   â”‚
    â”‚Sentinel2â”‚        â”‚ Prithvi  â”‚        â”‚ Segment â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ US-010  â”‚        â”‚  NDVI   â”‚        â”‚ OUTPUT  â”‚
    â”‚ Classifyâ”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Calculateâ”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Generatorâ”‚
    â”‚ Objects â”‚        â”‚ Stress  â”‚        â”‚JSON/TIF â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTPUT FINAL:
â”œâ”€â”€ analysis_results.json      (Structured data)
â”œâ”€â”€ semantic_map.tif           (GeoTIFF 3 layers)
â”œâ”€â”€ visualization.png          (300 DPI multi-panel)
â””â”€â”€ report.html (opcional)     (Interactive report)
```

### Pipeline Detallado

```
STEP 1: INITIALIZATION (0.1s)
â”œâ”€â”€ Parse input parameters (bbox, date, thresholds)
â”œâ”€â”€ Validate bbox (must be valid WGS84 coordinates)
â”œâ”€â”€ Create output directory with timestamp
â””â”€â”€ Initialize logger

STEP 2: DATA ACQUISITION (5-10s)
â”œâ”€â”€ Download Sentinel-2 L2A from Sentinel Hub
â”‚   â”œâ”€â”€ Bands: B02, B03, B04, B8A, B11, B12 (HLS format)
â”‚   â””â”€â”€ Resolution: Resample all to 10m
â”œâ”€â”€ Download SCL (Scene Classification Layer) for clouds
â””â”€â”€ Save raw data to cache (for future re-runs)

STEP 3: EMBEDDING EXTRACTION (10-15s)
â”œâ”€â”€ Load Prithvi model (cached after first run)
â”œâ”€â”€ Prepare HLS input (6 bands, normalized)
â”œâ”€â”€ Forward pass through encoder
â”œâ”€â”€ Extract 256D embeddings per pixel
â””â”€â”€ Save embeddings.npy (H, W, 256)

STEP 4: SEGMENTATION (3-5s)
â”œâ”€â”€ Initialize MGRG with threshold=0.95
â”œâ”€â”€ Generate grid seeds (20x20 spacing)
â”œâ”€â”€ Run region growing with cosine similarity
â”œâ”€â”€ Filter regions < 50 pixels
â”œâ”€â”€ Save segmentation.npy (H, W) with region IDs
â””â”€â”€ Calculate metrics: coherence, region count

STEP 5: CLASSIFICATION (1-2s)
â”œâ”€â”€ Initialize SemanticClassifier with embeddings + NDVI (US-010)
â”œâ”€â”€ Classify all regions (23-180 regions â†’ 6 classes bilingÃ¼es)
â”œâ”€â”€ Generate semantic map (H, W) with class IDs
â”œâ”€â”€ Calculate class statistics (area, count, mean NDVI)
â”œâ”€â”€ Nombres bilingÃ¼es: "Vigorous Crop (Cultivo Vigoroso)", etc.
â””â”€â”€ Save classification_results.json

STEP 6: STRESS ANALYSIS (1s)
â”œâ”€â”€ Filter only crop regions (class_id 3 or 4: Vigorous/Stressed Crop)
â”œâ”€â”€ Calculate NDVI statistics per crop region
â”œâ”€â”€ Assign stress level based on NDVI thresholds:
â”‚   â”œâ”€â”€ Low stress: NDVI >= 0.55 (saludable)
â”‚   â”œâ”€â”€ Medium stress: 0.40 <= NDVI < 0.55 (moderado)
â”‚   â””â”€â”€ High stress: NDVI < 0.40 (severo)
â”œâ”€â”€ Generate stress map (only crops colored, resto transparente)
â””â”€â”€ Save stress_analysis.json with regions grouped by level

STEP 7: OUTPUT GENERATION (2-3s)
â”œâ”€â”€ Generate GeoTIFF with 3 layers (si export_formats incluye "tif")
â”‚   â”œâ”€â”€ Layer 1: Segmentation (region IDs 0-N)
â”‚   â”œâ”€â”€ Layer 2: Classification (class IDs 0-5)
â”‚   â””â”€â”€ Layer 3: Stress levels (0=none, 1=low, 2=medium, 3=high)
â”œâ”€â”€ Generate visualizations PNG 300 DPI (si export_formats incluye "png")
â”‚   â”œâ”€â”€ Panel 1: RGB original (Sentinel-2 B04,B03,B02)
â”‚   â”œâ”€â”€ Panel 2: Semantic map (colored con CLASS_COLORS de US-010)
â”‚   â”œâ”€â”€ Panel 3: Stress map (solo cultivos coloreados)
â”‚   â””â”€â”€ Panel 4: Statistics table (hectÃ¡reas por clase)
â”œâ”€â”€ Generate final JSON with all results (SIEMPRE generado)
â”‚   â”œâ”€â”€ Metadata (bbox, date, zone, resolution, threshold)
â”‚   â”œâ”€â”€ Segmentation metrics (regions, coherence, time)
â”‚   â”œâ”€â”€ Classification results (lista de objetos con nombres bilingÃ¼es)
â”‚   â”œâ”€â”€ Stress analysis (agrupado por nivel: low/medium/high)
â”‚   â”œâ”€â”€ Summary (hectÃ¡reas totales por clase)
â”‚   â””â”€â”€ Processing times (por paso y total)
â””â”€â”€ Generate HTML report (opcional, si export_formats incluye "html")

TOTAL TIME: ~25-40 seconds (bbox pequeÃ±o con datos cacheados: ~15-20s)
```

---

## ğŸ’» EspecificaciÃ³n TÃ©cnica

### MÃ³dulo Core: `src/pipeline/hierarchical_analysis.py`

```python
"""
End-to-end hierarchical analysis pipeline.

Orchestrates all US components from Sentinel-2 download to stress analysis.
Shared logic between API REST and CLI script.

References:
    - US-003: Sentinel-2 download
    - US-006: Prithvi embeddings
    - US-007: MGRG segmentation
    - US-010: Semantic classification
"""

import numpy as np
from pathlib import Path
from typing import Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import logging
import json
import time

from src.data.sentinel_downloader import download_sentinel2_hls
from src.models.prithvi_wrapper import PrithviModel, extract_embeddings
from src.algorithms.semantic_region_growing import SemanticRegionGrowing
from src.classification.zero_shot_classifier import SemanticClassifier
from src.utils.ndvi_calculator import calculate_ndvi
from src.visualization.report_generator import generate_visualization, generate_geotiff

logger = logging.getLogger(__name__)


@dataclass
class AnalysisConfig:
    """Configuration for hierarchical analysis."""
    bbox: Tuple[float, float, float, float]  # (min_lon, min_lat, max_lon, max_lat)
    date_from: str  # YYYY-MM-DD
    date_to: Optional[str] = None  # If None, use date_from
    mgrg_threshold: float = 0.95
    min_region_size: int = 50
    resolution: float = 10.0  # meters
    output_dir: str = "output/analysis"
    export_formats: list = None  # ["json", "tif", "png", "html"]

    def __post_init__(self):
        if self.export_formats is None:
            self.export_formats = ["json", "tif", "png"]
        if self.date_to is None:
            self.date_to = self.date_from


@dataclass
class AnalysisResult:
    """Complete result of hierarchical analysis."""
    metadata: Dict
    segmentation: Dict
    classification: list
    stress_analysis: Dict
    summary: Dict
    output_files: Dict


class HierarchicalAnalysisPipeline:
    """
    Complete pipeline for hierarchical land cover and stress analysis.

    Parameters
    ----------
    config : AnalysisConfig
        Configuration object with all parameters

    Examples
    --------
    >>> config = AnalysisConfig(
    ...     bbox=(-115.35, 32.45, -115.25, 32.55),
    ...     date_from="2025-10-15"
    ... )
    >>> pipeline = HierarchicalAnalysisPipeline(config)
    >>> result = pipeline.run()
    >>> print(result.summary['vigorous_crop_ha'])
    1245.8
    """

    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize timer
        self.start_time = time.time()
        self.step_times = {}

        logger.info(f"HierarchicalAnalysisPipeline initialized")
        logger.info(f"BBox: {config.bbox}")
        logger.info(f"Date: {config.date_from}")
        logger.info(f"Output: {self.output_dir}")

    def _time_step(self, step_name: str):
        """Record time for a pipeline step."""
        elapsed = time.time() - self.start_time
        self.step_times[step_name] = elapsed
        logger.info(f"[{step_name}] Completed in {elapsed:.2f}s")

    def run(self) -> AnalysisResult:
        """
        Execute complete analysis pipeline.

        Returns
        -------
        AnalysisResult
            Complete results with all outputs

        Raises
        ------
        ValueError
            If bbox or date is invalid
        RuntimeError
            If any pipeline step fails
        """
        logger.info("=" * 60)
        logger.info("Starting Hierarchical Analysis Pipeline")
        logger.info("=" * 60)

        try:
            # Step 1: Data Acquisition
            logger.info("\n[STEP 1/6] Data Acquisition")
            hls_data, metadata = self._download_sentinel2()
            self._time_step("download")

            # Step 2: Embedding Extraction
            logger.info("\n[STEP 2/6] Extracting Prithvi Embeddings")
            embeddings = self._extract_embeddings(hls_data)
            self._time_step("embeddings")

            # Step 3: Segmentation
            logger.info("\n[STEP 3/6] MGRG Segmentation")
            segmentation, seg_metrics = self._segment(embeddings)
            self._time_step("segmentation")

            # Step 4: NDVI Calculation
            logger.info("\n[STEP 4/6] Calculating NDVI")
            ndvi = self._calculate_ndvi(hls_data)
            self._time_step("ndvi")

            # Step 5: Classification
            logger.info("\n[STEP 5/6] Semantic Classification")
            classifications, semantic_map = self._classify(
                embeddings, ndvi, segmentation
            )
            self._time_step("classification")

            # Step 6: Stress Analysis
            logger.info("\n[STEP 6/6] Stress Analysis")
            stress_results = self._analyze_stress(classifications, ndvi)
            self._time_step("stress")

            # Generate outputs
            logger.info("\n[OUTPUT] Generating Files")
            output_files = self._generate_outputs(
                hls_data, segmentation, semantic_map,
                classifications, stress_results
            )
            self._time_step("output")

            # Compile results
            result = self._compile_results(
                metadata, seg_metrics, classifications,
                stress_results, output_files
            )

            total_time = time.time() - self.start_time
            logger.info(f"\n{'=' * 60}")
            logger.info(f"Pipeline completed in {total_time:.2f}s")
            logger.info(f"{'=' * 60}")

            return result

        except Exception as e:
            logger.error(f"Pipeline failed: {e}", exc_info=True)
            raise RuntimeError(f"Analysis pipeline failed: {e}")

    def _download_sentinel2(self) -> Tuple[np.ndarray, Dict]:
        """Download Sentinel-2 HLS data."""
        hls_data, metadata = download_sentinel2_hls(
            bbox=self.config.bbox,
            date_from=self.config.date_from,
            date_to=self.config.date_to,
            bands=['B02', 'B03', 'B04', 'B8A', 'B11', 'B12'],
            resolution=self.config.resolution
        )

        logger.info(f"Downloaded HLS data: {hls_data.shape}")
        return hls_data, metadata

    def _extract_embeddings(self, hls_data: np.ndarray) -> np.ndarray:
        """Extract Prithvi embeddings."""
        model = PrithviModel()  # Singleton, cached
        embeddings = extract_embeddings(model, hls_data)

        # Save embeddings (useful for debugging)
        emb_path = self.output_dir / "embeddings.npy"
        np.save(emb_path, embeddings)

        logger.info(f"Embeddings extracted: {embeddings.shape}")
        return embeddings

    def _segment(self, embeddings: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Run MGRG segmentation."""
        mgrg = SemanticRegionGrowing(
            threshold=self.config.mgrg_threshold,
            min_region_size=self.config.min_region_size,
            use_smart_seeds=False  # Grid is superior (US-007)
        )

        segmentation = mgrg.segment(embeddings)

        # Calculate metrics
        from src.utils.comparison_metrics import (
            calculate_spatial_coherence,
            count_regions
        )

        metrics = {
            'method': 'MGRG',
            'threshold': self.config.mgrg_threshold,
            'regions': count_regions(segmentation),
            'coherence': calculate_spatial_coherence(segmentation),
        }

        # Save segmentation
        seg_path = self.output_dir / "segmentation.npy"
        np.save(seg_path, segmentation)

        logger.info(f"Segmentation: {metrics['regions']} regions, "
                   f"{metrics['coherence']:.1f}% coherence")

        return segmentation, metrics

    def _calculate_ndvi(self, hls_data: np.ndarray) -> np.ndarray:
        """Calculate NDVI from HLS bands."""
        # HLS bands: [B02, B03, B04, B8A, B11, B12]
        # NDVI = (NIR - Red) / (NIR + Red)
        # NIR = B8A (index 3), Red = B04 (index 2)
        nir = hls_data[:, :, 3]
        red = hls_data[:, :, 2]

        ndvi = calculate_ndvi(nir, red)

        # Save NDVI
        ndvi_path = self.output_dir / "ndvi.npy"
        np.save(ndvi_path, ndvi)

        logger.info(f"NDVI calculated: mean={ndvi.mean():.3f}, "
                   f"std={ndvi.std():.3f}")

        return ndvi

    def _classify(
        self,
        embeddings: np.ndarray,
        ndvi: np.ndarray,
        segmentation: np.ndarray
    ) -> Tuple[Dict, np.ndarray]:
        """Classify all regions using US-010 SemanticClassifier."""
        from src.classification.zero_shot_classifier import SemanticClassifier
        
        classifier = SemanticClassifier(
            embeddings, ndvi, resolution=self.config.resolution
        )

        # Classify all regions (returns Dict[int, ClassificationResult])
        classifications = classifier.classify_all_regions(
            segmentation, 
            min_size=self.config.min_region_size
        )
        
        # Generate semantic map with class IDs
        semantic_map = classifier.generate_semantic_map(
            segmentation, classifications
        )

        # Save semantic map
        sem_path = self.output_dir / "semantic_map.npy"
        np.save(sem_path, semantic_map)

        logger.info(f"Classified {len(classifications)} regions")
        logger.info(f"Classes found: {set(r.class_name for r in classifications.values())}")

        return classifications, semantic_map

    def _analyze_stress(
        self,
        classifications: Dict,
        ndvi: np.ndarray
    ) -> Dict:
        """
        Analyze stress for crop regions only.
        
        Stress levels based on NDVI thresholds:
        - Low stress: NDVI >= 0.55 (healthy vegetation)
        - Medium stress: 0.40 <= NDVI < 0.55 (moderate stress)
        - High stress: NDVI < 0.40 (severe stress)
        
        Only analyzes regions classified as crops (class_id 3 or 4).
        """
        stress_results = {
            'low': {'count': 0, 'area_ha': 0.0, 'regions': []},
            'medium': {'count': 0, 'area_ha': 0.0, 'regions': []},
            'high': {'count': 0, 'area_ha': 0.0, 'regions': []},
        }

        crop_count = 0
        for region_id, result in classifications.items():
            # Only analyze crops (class_id 3: Vigorous Crop, 4: Stressed Crop)
            if result.class_id not in [3, 4]:
                continue

            crop_count += 1
            mean_ndvi = result.mean_ndvi

            # Assign stress level based on NDVI thresholds
            if mean_ndvi >= 0.55:
                level = 'low'
            elif mean_ndvi >= 0.40:
                level = 'medium'
            else:
                level = 'high'

            stress_results[level]['count'] += 1
            stress_results[level]['area_ha'] += result.area_hectares
            stress_results[level]['regions'].append({
                'region_id': region_id,
                'class_name': result.class_name,
                'ndvi': mean_ndvi,
                'area_ha': result.area_hectares
            })

        logger.info(f"Stress analysis on {crop_count} crop regions: "
                   f"Low={stress_results['low']['count']}, "
                   f"Medium={stress_results['medium']['count']}, "
                   f"High={stress_results['high']['count']}")

        return stress_results

    def _generate_outputs(
        self,
        hls_data: np.ndarray,
        segmentation: np.ndarray,
        semantic_map: np.ndarray,
        classifications: Dict,
        stress_results: Dict
    ) -> Dict[str, str]:
        """Generate all output files."""
        output_files = {}

        # 1. JSON (always generated)
        if "json" in self.config.export_formats:
            json_path = self.output_dir / "analysis_results.json"
            self._save_json(classifications, stress_results, json_path)
            output_files['json'] = str(json_path)
            logger.info(f"Generated JSON: {json_path}")

        # 2. GeoTIFF (optional)
        if "tif" in self.config.export_formats:
            tif_path = self.output_dir / "semantic_map.tif"
            generate_geotiff(
                segmentation, semantic_map,
                self.config.bbox, tif_path
            )
            output_files['tif'] = str(tif_path)
            logger.info(f"Generated GeoTIFF: {tif_path}")

        # 3. PNG Visualization (optional)
        if "png" in self.config.export_formats:
            png_path = self.output_dir / "visualization.png"
            rgb = hls_data[:, :, [2, 1, 0]]  # B04, B03, B02 â†’ RGB
            generate_visualization(
                rgb, semantic_map, classifications,
                stress_results, png_path
            )
            output_files['png'] = str(png_path)
            logger.info(f"Generated PNG: {png_path}")

        # 4. HTML Report (optional)
        if "html" in self.config.export_formats:
            html_path = self.output_dir / "report.html"
            # TODO: Implement HTML report generator
            output_files['html'] = str(html_path)

        return output_files

    def _save_json(
        self,
        classifications: Dict,
        stress_results: Dict,
        output_path: Path
    ):
        """Save results as structured JSON."""
        # Convert classifications to list of dicts
        classification_list = [
            {
                'region_id': int(region_id),
                'class': result.class_name,
                'class_id': result.class_id,
                'confidence': result.confidence,
                'area_ha': result.area_hectares,
                'mean_ndvi': result.mean_ndvi,
                'std_ndvi': result.std_ndvi,
            }
            for region_id, result in classifications.items()
        ]

        # Calculate summary
        from collections import defaultdict
        summary = defaultdict(float)
        for result in classifications.values():
            summary[f"{result.class_name.lower().replace(' ', '_')}_ha"] += \
                result.area_hectares

        # Compile final JSON
        output_data = {
            'metadata': {
                'bbox': list(self.config.bbox),
                'date_from': self.config.date_from,
                'date_to': self.config.date_to,
                'resolution': self.config.resolution,
                'mgrg_threshold': self.config.mgrg_threshold,
            },
            'segmentation': {
                'method': 'MGRG',
                'regions': len(classifications),
            },
            'classification': classification_list,
            'stress_analysis': stress_results,
            'summary': dict(summary),
            'processing_time': self.step_times,
        }

        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2)

    def _compile_results(
        self,
        metadata: Dict,
        seg_metrics: Dict,
        classifications: Dict,
        stress_results: Dict,
        output_files: Dict
    ) -> AnalysisResult:
        """Compile all results into AnalysisResult object."""
        # Convert classifications to list
        classification_list = [
            asdict(result) for result in classifications.values()
        ]

        # Calculate summary
        from collections import defaultdict
        summary = defaultdict(float)
        for result in classifications.values():
            key = f"{result.class_name.lower().replace(' ', '_')}_ha"
            summary[key] += result.area_hectares

        return AnalysisResult(
            metadata=metadata,
            segmentation=seg_metrics,
            classification=classification_list,
            stress_analysis=stress_results,
            summary=dict(summary),
            output_files=output_files
        )
```

### API REST: `backend/app/api/routes/hierarchical.py`

```python
"""
FastAPI endpoint for hierarchical analysis.

Usage:
    POST /api/analysis/hierarchical
    Body: {
        "bbox": [-115.35, 32.45, -115.25, 32.55],
        "date_from": "2025-10-15",
        "mgrg_threshold": 0.95,
        "export_formats": ["json", "tif", "png"]
    }
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Tuple
import uuid
from pathlib import Path

from src.pipeline.hierarchical_analysis import (
    HierarchicalAnalysisPipeline,
    AnalysisConfig,
    AnalysisResult
)

router = APIRouter(prefix="/api/analysis", tags=["analysis"])


class HierarchicalAnalysisRequest(BaseModel):
    """Request schema for hierarchical analysis."""
    bbox: Tuple[float, float, float, float] = Field(
        ...,
        description="Bounding box (min_lon, min_lat, max_lon, max_lat)"
    )
    date_from: str = Field(..., description="Start date YYYY-MM-DD")
    date_to: Optional[str] = Field(None, description="End date (optional)")
    mgrg_threshold: float = Field(0.95, ge=0.7, le=0.99)
    min_region_size: int = Field(50, ge=10, le=500)
    export_formats: List[str] = Field(["json", "tif", "png"])

    @validator('bbox')
    def validate_bbox(cls, v):
        if not (-180 <= v[0] < v[2] <= 180 and -90 <= v[1] < v[3] <= 90):
            raise ValueError("Invalid bbox coordinates")
        return v

    @validator('date_from', 'date_to')
    def validate_date(cls, v):
        if v is None:
            return v
        # Simple validation (can be improved)
        if not (len(v) == 10 and v[4] == '-' and v[7] == '-'):
            raise ValueError("Date must be YYYY-MM-DD format")
        return v


class HierarchicalAnalysisResponse(BaseModel):
    """Response schema for hierarchical analysis."""
    analysis_id: str
    status: str  # "processing" | "completed" | "failed"
    message: str
    output_files: Optional[dict] = None


# In-memory storage (replace with Redis/DB in production)
analysis_status = {}


@router.post("/hierarchical", response_model=HierarchicalAnalysisResponse)
async def run_hierarchical_analysis(
    request: HierarchicalAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    Run complete hierarchical analysis pipeline.

    Returns immediately with analysis_id.
    Processing runs in background (async).
    """
    # Generate unique ID
    analysis_id = str(uuid.uuid4())

    # Create config
    config = AnalysisConfig(
        bbox=request.bbox,
        date_from=request.date_from,
        date_to=request.date_to,
        mgrg_threshold=request.mgrg_threshold,
        min_region_size=request.min_region_size,
        export_formats=request.export_formats,
        output_dir=f"output/api/{analysis_id}"
    )

    # Initialize status
    analysis_status[analysis_id] = {
        'status': 'processing',
        'message': 'Analysis started',
    }

    # Run pipeline in background
    background_tasks.add_task(
        run_pipeline_background,
        analysis_id,
        config
    )

    return HierarchicalAnalysisResponse(
        analysis_id=analysis_id,
        status="processing",
        message="Analysis started. Use GET /analysis/{id}/status to check progress."
    )


async def run_pipeline_background(analysis_id: str, config: AnalysisConfig):
    """Run pipeline in background task."""
    try:
        pipeline = HierarchicalAnalysisPipeline(config)
        result = pipeline.run()

        # Update status
        analysis_status[analysis_id] = {
            'status': 'completed',
            'message': 'Analysis completed successfully',
            'output_files': result.output_files,
            'summary': result.summary,
        }

    except Exception as e:
        analysis_status[analysis_id] = {
            'status': 'failed',
            'message': f"Analysis failed: {str(e)}",
        }


@router.get("/hierarchical/{analysis_id}/status")
async def get_analysis_status(analysis_id: str):
    """Get status of analysis."""
    if analysis_id not in analysis_status:
        raise HTTPException(status_code=404, detail="Analysis not found")

    return analysis_status[analysis_id]


@router.get("/hierarchical/{analysis_id}/download/{file_type}")
async def download_analysis_result(analysis_id: str, file_type: str):
    """Download result file (json, tif, png)."""
    if analysis_id not in analysis_status:
        raise HTTPException(status_code=404, detail="Analysis not found")

    status = analysis_status[analysis_id]
    if status['status'] != 'completed':
        raise HTTPException(status_code=400, detail="Analysis not completed")

    # Get file path
    output_files = status.get('output_files', {})
    file_path = output_files.get(file_type)

    if not file_path or not Path(file_path).exists():
        raise HTTPException(status_code=404, detail=f"File type {file_type} not found")

    # Determine media type
    media_types = {
        'json': 'application/json',
        'tif': 'image/tiff',
        'png': 'image/png',
        'html': 'text/html',
    }

    return FileResponse(
        file_path,
        media_type=media_types.get(file_type, 'application/octet-stream'),
        filename=Path(file_path).name
    )
```

### CLI Script: `scripts/analyze_region.py`

```python
#!/usr/bin/env python3
"""
Command-line interface for hierarchical analysis.

Usage:
    python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15"
    python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15" --output "output/mexicali"
    python analyze_region.py --help

Examples:
    # Basic analysis
    python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15"

    # Custom threshold
    python analyze_region.py --bbox "..." --date "..." --threshold 0.90

    # Export specific formats
    python analyze_region.py --bbox "..." --date "..." --formats json,tif

    # Verbose logging
    python analyze_region.py --bbox "..." --date "..." --verbose
"""

import argparse
import sys
import logging
from pathlib import Path
from typing import Tuple

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.pipeline.hierarchical_analysis import (
    HierarchicalAnalysisPipeline,
    AnalysisConfig
)


def parse_bbox(bbox_str: str) -> Tuple[float, float, float, float]:
    """Parse bbox string to tuple."""
    try:
        coords = [float(x) for x in bbox_str.split(',')]
        if len(coords) != 4:
            raise ValueError("BBox must have 4 coordinates")
        return tuple(coords)
    except Exception as e:
        raise ValueError(f"Invalid bbox format: {e}")


def setup_logging(verbose: bool = False):
    """Setup logging configuration."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('analysis.log')
        ]
    )


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Hierarchical Analysis CLI - Region Growing System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze Mexicali region
  python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15"

  # Custom output directory
  python analyze_region.py --bbox "..." --date "..." --output "results/mexicali_20251015"

  # Export only JSON and PNG
  python analyze_region.py --bbox "..." --date "..." --formats json,png

  # Adjust MGRG threshold
  python analyze_region.py --bbox "..." --date "..." --threshold 0.90

For more information: https://github.com/your-repo/region-growing
        """
    )

    # Required arguments
    parser.add_argument(
        '--bbox',
        type=str,
        required=True,
        help='Bounding box as "min_lat,min_lon,max_lat,max_lon" (WGS84)'
    )
    parser.add_argument(
        '--date',
        type=str,
        required=True,
        help='Analysis date as YYYY-MM-DD (e.g., 2025-10-15)'
    )

    # Optional arguments
    parser.add_argument(
        '--output',
        type=str,
        default='output/cli',
        help='Output directory (default: output/cli)'
    )
    parser.add_argument(
        '--threshold',
        type=float,
        default=0.95,
        help='MGRG similarity threshold 0.7-0.99 (default: 0.95)'
    )
    parser.add_argument(
        '--min-size',
        type=int,
        default=50,
        help='Minimum region size in pixels (default: 50)'
    )
    parser.add_argument(
        '--formats',
        type=str,
        default='json,tif,png',
        help='Export formats comma-separated: json,tif,png,html (default: json,tif,png)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )

    args = parser.parse_args()

    # Setup logging
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)

    try:
        # Parse inputs
        bbox = parse_bbox(args.bbox)
        export_formats = args.formats.split(',')

        # Create config
        config = AnalysisConfig(
            bbox=bbox,
            date_from=args.date,
            mgrg_threshold=args.threshold,
            min_region_size=args.min_size,
            output_dir=args.output,
            export_formats=export_formats
        )

        logger.info("="*60)
        logger.info("Hierarchical Analysis CLI")
        logger.info("="*60)
        logger.info(f"BBox: {bbox}")
        logger.info(f"Date: {args.date}")
        logger.info(f"Output: {args.output}")
        logger.info(f"Threshold: {args.threshold}")
        logger.info("="*60)

        # Run pipeline
        pipeline = HierarchicalAnalysisPipeline(config)
        result = pipeline.run()

        # Print results
        print("\n" + "="*60)
        print("ANALYSIS COMPLETE")
        print("="*60)
        print(f"\nOutput files:")
        for file_type, file_path in result.output_files.items():
            print(f"  {file_type.upper()}: {file_path}")

        print(f"\nSummary:")
        for key, value in result.summary.items():
            print(f"  {key}: {value:.1f} ha")

        print("\n" + "="*60)

        return 0  # Success

    except KeyboardInterrupt:
        logger.warning("Analysis interrupted by user")
        return 130  # SIGINT

    except Exception as e:
        logger.error(f"Analysis failed: {e}", exc_info=True)
        print(f"\nERROR: {e}", file=sys.stderr)
        print("Use --verbose for detailed error information", file=sys.stderr)
        return 1  # Error


if __name__ == "__main__":
    sys.exit(main())
```

---

## ğŸ“Š Plan de Desarrollo

### Fase 1: Core Pipeline (3 horas)
1. **Crear `src/pipeline/hierarchical_analysis.py`** (2h)
   - Clase `HierarchicalAnalysisPipeline` con 6 pasos
   - Dataclasses: `AnalysisConfig`, `AnalysisResult`
   - OrquestaciÃ³n de mÃ³dulos existentes (US-003, US-006, US-007, US-010)
   - Manejo de errores con try/except y logging detallado
   - Timer por paso para mÃ©tricas de performance
   - ValidaciÃ³n de datos intermedios (shapes, tipos)

2. **Tests Unitarios Pipeline** (1h)
   - Mock de componentes externos (Sentinel Hub, Prithvi)
   - Test happy path con datos sintÃ©ticos
   - Test error handling (bbox invÃ¡lido, sin datos, timeout)
   - Test de validaciÃ³n de outputs (JSON, TIF, PNG)
   - Cobertura >70%

### Fase 2: API REST (3 horas)
1. **Endpoint POST /api/analysis/hierarchical** (1.5h)
   - Schema Pydantic: `HierarchicalAnalysisRequest`, `HierarchicalAnalysisResponse`
   - ValidaciÃ³n de bbox (WGS84, lÃ­mites vÃ¡lidos)
   - ValidaciÃ³n de fecha (formato YYYY-MM-DD)
   - Background task con FastAPI BackgroundTasks
   - Status storage en memoria (dict global para MVP)
   - GeneraciÃ³n de analysis_id Ãºnico (UUID)

2. **Endpoints auxiliares** (1h)
   - GET /api/analysis/hierarchical/{analysis_id}/status
   - GET /api/analysis/hierarchical/{analysis_id}/download/{file_type}
   - Error handling con HTTPException (400, 404, 500)
   - CÃ³digos de respuesta apropiados
   - DocumentaciÃ³n en Swagger UI

3. **ConfiguraciÃ³n y Testing** (30 min)
   - CORS para frontend Nuxt 3
   - Logging de requests
   - Test de endpoints con pytest + TestClient
   - Mock de pipeline para tests rÃ¡pidos

### Fase 3: CLI Script (2 horas)
1. **Script `scripts/analyze_region.py`** (1.5h)
   - Argumentos con argparse (--bbox, --date, --output, --threshold, --formats, --verbose)
   - Parsing de bbox desde string "lat1,lon1,lat2,lon2"
   - ValidaciÃ³n de argumentos
   - Llamada a `HierarchicalAnalysisPipeline`
   - Progress bar con tqdm (opcional, puede ser logging)
   - Exit codes informativos (0=success, 1=error, 130=interrupted)
   - Logging a archivo y consola
   - Manejo de KeyboardInterrupt

2. **DocumentaciÃ³n y Ejemplos** (30 min)
   - Help text detallado con ejemplos
   - Epilog con casos de uso comunes
   - README actualizado con secciÃ³n CLI
   - Ejemplos para Mexicali, BajÃ­o, Sinaloa

### Fase 4: Testing e IntegraciÃ³n (30 min)
1. **Tests End-to-End** (30 min)
   - Test CLI con bbox vÃ¡lido
   - Test API con mock request
   - Verificar outputs generados

---

## ğŸ§ª Testing

```python
# tests/integration/test_hierarchical_pipeline.py

def test_cli_script_success():
    """CLI should complete successfully with valid inputs"""
    result = subprocess.run([
        'python', 'scripts/analyze_region.py',
        '--bbox', '32.45,-115.35,32.55,-115.25',
        '--date', '2025-10-15',
        '--output', 'tmp/test_output'
    ], capture_output=True)

    assert result.returncode == 0
    assert Path('tmp/test_output/analysis_results.json').exists()

def test_api_endpoint_success(client):
    """API should return analysis_id and process in background"""
    response = client.post('/api/analysis/hierarchical', json={
        'bbox': [32.45, -115.35, 32.55, -115.25],
        'date_from': '2025-10-15'
    })

    assert response.status_code == 200
    data = response.json()
    assert 'analysis_id' in data
    assert data['status'] == 'processing'
```

---

## ğŸ“š Lecciones Aprendidas de US Anteriores

### De US-009 (ValidaciÃ³n con Ground Truth)
- **ValidaciÃ³n de datos crÃ­tica**: Implementar verificaciÃ³n de shapes y formatos antes de procesamiento
- **Fallback automÃ¡tico Ãºtil**: Tener datos sintÃ©ticos para testing rÃ¡pido sin dependencias externas
- **MÃ©tricas agregadas importantes**: Incluir desviaciÃ³n estÃ¡ndar y estadÃ­sticas por zona
- **Helpers reutilizables**: Crear funciones en `src/utils/` para cargar y validar datos

### De US-010 (ClasificaciÃ³n SemÃ¡ntica)
- **Nombres bilingÃ¼es mejoran interpretabilidad**: Usar formato "English (EspaÃ±ol)" en outputs
- **VerificaciÃ³n de datos primero**: Script de verificaciÃ³n ahorra tiempo de debugging
- **Tests incrementales**: Test simple antes de pipeline completo
- **Cross-validation esencial**: Validar con Dynamic World para verificar resultados
- **DocumentaciÃ³n de thresholds**: Documentar claramente los umbrales de NDVI y su justificaciÃ³n

### Aplicaciones a US-011
1. **ValidaciÃ³n de datos intermedios**: Verificar que NDVI, embeddings, segmentaciÃ³n existen antes de clasificar
2. **Nombres bilingÃ¼es en JSON**: Usar formato de US-010 para clases
3. **Logging detallado**: Logger profesional en lugar de prints
4. **Tests con mocks**: No depender de Sentinel Hub para tests unitarios
5. **DocumentaciÃ³n clara**: Docstrings estilo Google en todas las funciones

---

## âš ï¸ Riesgos y Mitigaciones

### Riesgo 1: Timeout en descarga Sentinel-2
**Probabilidad**: Media  
**Impacto**: Alto (bloquea todo el pipeline)  
**MitigaciÃ³n**: 
- Timeout de 5 min en requests
- Retry con exponential backoff (3 intentos)
- Cache de imÃ¡genes descargadas para re-ejecuciones
- Mensaje de error claro con sugerencias

### Riesgo 2: Memoria insuficiente para imÃ¡genes grandes
**Probabilidad**: Media  
**Impacto**: Alto (crash del proceso)  
**MitigaciÃ³n**: 
- Limitar bbox mÃ¡ximo (0.1Â° x 0.1Â° ~10km x 10km)
- ValidaciÃ³n de tamaÃ±o antes de descarga
- Procesamiento por tiles si bbox > lÃ­mite
- Liberar memoria despuÃ©s de cada paso (del variables)

### Riesgo 3: Datos intermedios faltantes
**Probabilidad**: Baja (si US anteriores funcionan)  
**Impacto**: Alto (pipeline falla a mitad)  
**MitigaciÃ³n**:
- VerificaciÃ³n de archivos al inicio de cada paso
- Mensajes de error claros indicando quÃ© falta
- OpciÃ³n de re-generar datos intermedios
- Logging de paths de archivos esperados

### Riesgo 4: API sobrecargada (muchos requests simultÃ¡neos)
**Probabilidad**: Baja (MVP con pocos usuarios)  
**Impacto**: Medio (degradaciÃ³n de performance)  
**MitigaciÃ³n**:
- Rate limiting 10 req/min por IP (opcional para MVP)
- Queue con Celery en producciÃ³n (no MVP)
- Timeout de 5 min por anÃ¡lisis
- Mensaje de "servidor ocupado" si >5 anÃ¡lisis activos

### Riesgo 5: Discrepancia entre API y CLI
**Probabilidad**: Media (cÃ³digo duplicado)  
**Impacto**: Medio (inconsistencia de resultados)  
**MitigaciÃ³n**:
- LÃ³gica compartida en `HierarchicalAnalysisPipeline`
- Tests que comparan outputs de API vs CLI
- Mismos parÃ¡metros por defecto
- DocumentaciÃ³n clara de diferencias (async vs sync)

---

## ğŸ¯ Mejoras Sugeridas para ImplementaciÃ³n

### 1. ValidaciÃ³n Robusta de Inputs
```python
# En HierarchicalAnalysisPipeline.__init__
def _validate_config(self):
    """Validate configuration before starting pipeline."""
    # Validate bbox
    min_lon, min_lat, max_lon, max_lat = self.config.bbox
    if not (-180 <= min_lon < max_lon <= 180):
        raise ValueError(f"Invalid longitude range: {min_lon}, {max_lon}")
    if not (-90 <= min_lat < max_lat <= 90):
        raise ValueError(f"Invalid latitude range: {min_lat}, {max_lat}")
    
    # Validate bbox size (max 0.1Â° x 0.1Â° ~10km x 10km)
    if (max_lon - min_lon) > 0.1 or (max_lat - min_lat) > 0.1:
        raise ValueError("BBox too large. Maximum size: 0.1Â° x 0.1Â°")
    
    # Validate date format
    try:
        datetime.strptime(self.config.date_from, "%Y-%m-%d")
    except ValueError:
        raise ValueError(f"Invalid date format: {self.config.date_from}. Use YYYY-MM-DD")
```

### 2. Cache de Datos Intermedios
```python
# En cada paso del pipeline
def _download_sentinel2(self) -> Tuple[np.ndarray, Dict]:
    """Download Sentinel-2 with caching."""
    cache_key = f"{self.config.bbox}_{self.config.date_from}"
    cache_path = Path("cache/sentinel2") / f"{cache_key}.npz"
    
    if cache_path.exists():
        logger.info(f"Loading cached Sentinel-2 data: {cache_path}")
        data = np.load(cache_path)
        return data['hls'], data['metadata'].item()
    
    # Download if not cached
    hls_data, metadata = download_sentinel2_hls(...)
    
    # Save to cache
    cache_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(cache_path, hls=hls_data, metadata=metadata)
    
    return hls_data, metadata
```

### 3. Progress Tracking para API
```python
# En run_pipeline_background
async def run_pipeline_background(analysis_id: str, config: AnalysisConfig):
    """Run pipeline with progress tracking."""
    try:
        # Update status with progress
        analysis_status[analysis_id]['progress'] = 0
        analysis_status[analysis_id]['current_step'] = 'Downloading Sentinel-2'
        
        pipeline = HierarchicalAnalysisPipeline(config)
        
        # Hook into pipeline steps to update progress
        # (requires modifying HierarchicalAnalysisPipeline to accept callbacks)
        
        result = pipeline.run()
        
        analysis_status[analysis_id] = {
            'status': 'completed',
            'progress': 100,
            'message': 'Analysis completed successfully',
            'output_files': result.output_files,
            'summary': result.summary,
        }
    except Exception as e:
        analysis_status[analysis_id] = {
            'status': 'failed',
            'progress': 0,
            'message': f"Analysis failed: {str(e)}",
        }
```

### 4. ExportaciÃ³n Flexible de Formatos
```python
# En _generate_outputs
def _generate_outputs(self, ...) -> Dict[str, str]:
    """Generate outputs based on export_formats config."""
    output_files = {}
    
    # JSON always generated (required for API response)
    json_path = self.output_dir / "analysis_results.json"
    self._save_json(classifications, stress_results, json_path)
    output_files['json'] = str(json_path)
    
    # Optional formats
    if "tif" in self.config.export_formats:
        tif_path = self.output_dir / "semantic_map.tif"
        generate_geotiff(segmentation, semantic_map, self.config.bbox, tif_path)
        output_files['tif'] = str(tif_path)
    
    if "png" in self.config.export_formats:
        png_path = self.output_dir / "visualization.png"
        generate_visualization(rgb, semantic_map, classifications, stress_results, png_path)
        output_files['png'] = str(png_path)
    
    if "html" in self.config.export_formats:
        html_path = self.output_dir / "report.html"
        generate_html_report(classifications, stress_results, html_path)
        output_files['html'] = str(html_path)
    
    return output_files
```

### 5. Logging Estructurado
```python
# En cada paso del pipeline
logger.info(
    "Step completed",
    extra={
        'step': 'download',
        'duration_s': elapsed,
        'bbox': self.config.bbox,
        'date': self.config.date_from,
        'data_shape': hls_data.shape
    }
)
```

---

## âœ… DefiniciÃ³n de Completado

### CÃ³digo
- [ ] `src/pipeline/hierarchical_analysis.py` implementado (456+ lÃ­neas)
- [ ] `src/pipeline/__init__.py` con exports
- [ ] `backend/app/api/routes/hierarchical.py` implementado (200+ lÃ­neas)
- [ ] `scripts/analyze_region.py` implementado (150+ lÃ­neas)
- [ ] Type hints en todas las funciones pÃºblicas
- [ ] Docstrings estilo Google en todas las clases y mÃ©todos
- [ ] Logging profesional con `logger` (no `print`)
- [ ] Sin emojis en cÃ³digo Python

### Testing
- [ ] Tests unitarios de `HierarchicalAnalysisPipeline` (>70% cobertura)
- [ ] Tests de integraciÃ³n API (POST, GET status, GET download)
- [ ] Tests de integraciÃ³n CLI (bbox vÃ¡lido, invÃ¡lido, timeout)
- [ ] Tests de validaciÃ³n de outputs (JSON, TIF, PNG)
- [ ] Mock de componentes externos (Sentinel Hub, Prithvi)

### DocumentaciÃ³n
- [ ] Docstrings completos en todas las funciones
- [ ] README actualizado con secciÃ³n API y CLI
- [ ] Ejemplos de uso en docstrings
- [ ] Swagger UI documentado automÃ¡ticamente
- [ ] Help text detallado en CLI

### Funcionalidad
- [ ] Pipeline completo funciona end-to-end
- [ ] API REST responde correctamente
- [ ] CLI ejecuta sin errores
- [ ] Outputs generados correctamente (JSON, TIF, PNG)
- [ ] Nombres bilingÃ¼es en outputs (segÃºn US-010)
- [ ] ValidaciÃ³n de inputs robusta
- [ ] Manejo de errores graceful
- [ ] Logging detallado por paso

### Cumplimiento AGENTS.md
- [ ] CÃ³digo en inglÃ©s, documentaciÃ³n en espaÃ±ol
- [ ] Funciones reutilizables en `src/pipeline/`
- [ ] No cÃ³digo duplicado entre API y CLI
- [ ] Un solo documento de resoluciÃ³n en `docs/us-resolved/us-011.md` al finalizar
- [ ] Sin archivos markdown adicionales durante desarrollo

---

## ğŸ’¡ Recomendaciones Finales para ImplementaciÃ³n

### Prioridades de Desarrollo
1. **Primero**: Core Pipeline (`HierarchicalAnalysisPipeline`) - Base compartida
2. **Segundo**: CLI Script - MÃ¡s simple, Ãºtil para testing
3. **Tercero**: API REST - Requiere CLI funcionando primero

### Decisiones de DiseÃ±o Clave

#### 1. Background Tasks: FastAPI BackgroundTasks vs Celery
**DecisiÃ³n**: Usar FastAPI BackgroundTasks para MVP
**RazÃ³n**: 
- MÃ¡s simple, sin dependencias externas (Redis, RabbitMQ)
- Suficiente para <10 usuarios concurrentes
- FÃ¡cil migraciÃ³n a Celery despuÃ©s si es necesario

#### 2. Storage de Status: Memoria vs Redis
**DecisiÃ³n**: Dict en memoria para MVP
**RazÃ³n**:
- MÃ¡s simple, sin setup adicional
- Suficiente para desarrollo y testing
- LimitaciÃ³n: Status se pierde al reiniciar servidor (documentar)

#### 3. Cache de Datos: Filesystem vs Database
**DecisiÃ³n**: Filesystem con numpy .npz
**RazÃ³n**:
- MÃ¡s rÃ¡pido para arrays grandes
- FÃ¡cil de implementar
- Compatible con estructura actual del proyecto

#### 4. ValidaciÃ³n de BBox: Estricta vs Permisiva
**DecisiÃ³n**: Estricta (max 0.1Â° x 0.1Â°)
**RazÃ³n**:
- Previene timeouts y crashes por memoria
- Fuerza al usuario a dividir Ã¡reas grandes
- Mejor experiencia de usuario (respuestas rÃ¡pidas)

### Orden de ImplementaciÃ³n Sugerido

**DÃ­a 1 (3 horas): Core Pipeline**
1. Crear estructura de archivos
2. Implementar `AnalysisConfig` y `AnalysisResult` dataclasses
3. Implementar `HierarchicalAnalysisPipeline.__init__` y `_validate_config`
4. Implementar mÃ©todos privados de orquestaciÃ³n (_download, _extract, etc.)
5. Implementar mÃ©todo pÃºblico `run()`
6. Tests unitarios con mocks

**DÃ­a 2 (3 horas): API REST**
1. Crear schemas Pydantic
2. Implementar endpoint POST /hierarchical
3. Implementar background task runner
4. Implementar endpoints GET /status y /download
5. Configurar CORS
6. Tests de integraciÃ³n con TestClient

**DÃ­a 3 (2 horas): CLI Script**
1. Implementar argparse con todos los argumentos
2. Implementar funciÃ³n `parse_bbox`
3. Implementar funciÃ³n `main` con try/except
4. Configurar logging
5. Tests de integraciÃ³n con subprocess
6. DocumentaciÃ³n y ejemplos

### Checklist Pre-ImplementaciÃ³n
- [ ] US-003, US-006, US-007, US-010 completadas y funcionando
- [ ] Datos de prueba disponibles (Mexicali, BajÃ­o, Sinaloa)
- [ ] Prithvi model descargado y funcional
- [ ] Sentinel Hub API key configurada
- [ ] Poetry environment actualizado con todas las dependencias
- [ ] Tests de componentes individuales pasando

### Checklist Post-ImplementaciÃ³n
- [ ] Pipeline ejecuta end-to-end sin errores
- [ ] API responde correctamente en Swagger UI
- [ ] CLI ejecuta con --help y muestra ejemplos
- [ ] Tests unitarios >70% cobertura
- [ ] Tests de integraciÃ³n pasando
- [ ] DocumentaciÃ³n actualizada
- [ ] Ejemplos probados manualmente
- [ ] CÃ³digo formateado con Black
- [ ] Sin warnings de Ruff
- [ ] Commit con mensaje descriptivo
- [ ] Documento de resoluciÃ³n en `docs/us-resolved/us-011.md`

---

**Autor**: Carlos Bocanegra (API), Arthur Zizumbo (Pipeline + CLI)  
**Fecha de CreaciÃ³n**: 11 de Noviembre de 2025  
**Ãšltima ActualizaciÃ³n**: 13 de Noviembre de 2025  
**VersiÃ³n**: 2.0 (Revisada con lecciones de US-009 y US-010)  
**Estado**: ğŸ“‹ LISTA PARA IMPLEMENTACIÃ“N
