# US-011: Sistema de An√°lisis Jer√°rquico End-to-End (API REST + CLI)

**Estado:** üìã PLANEACI√ìN
**Fecha de Creaci√≥n:** 11 de Noviembre de 2025
**Equipo:** 24 - Region Growing
**Desarrollador Asignado:** Carlos Bocanegra (Backend API), Arthur Zizumbo (Algoritmo + CLI)
**Estimaci√≥n:** 6 horas (4h API + 2h CLI + integraci√≥n)
**Prioridad:** ALTA (Entregable final del sistema)

---

## üìã Historia de Usuario

**Como** usuario final (agr√≥nomo, investigador, desarrollador)
**Quiero** un sistema completo que me permita ejecutar el pipeline MGRG desde imagen hasta an√°lisis de estr√©s
**Para que** pueda obtener resultados interpretables (objetos clasificados con an√°lisis de estr√©s) mediante:
- **API REST** para integraci√≥n en aplicaciones web/m√≥viles
- **CLI script** para uso en terminal, notebooks, o automatizaci√≥n

---

## üéØ Contexto y Motivaci√≥n

### Problema Actual
- **US-001 a US-010 son componentes individuales**: Requieren ejecuci√≥n manual paso a paso
- **Sin orquestaci√≥n**: Usuario debe conocer el flujo completo
- **Dif√≠cil integraci√≥n**: No hay interfaz unificada para sistemas externos
- **No reproducible f√°cilmente**: Comandos complejos en notebooks

### Soluci√≥n Propuesta
**Sistema Dual: API REST + CLI Script**

#### Opci√≥n 1: API REST (FastAPI)
- **Para**: Aplicaciones web, dashboards, servicios cloud
- **Ventajas**: As√≠ncrono, escalable, documentaci√≥n autom√°tica (Swagger)
- **Uso**: `POST /api/analysis/hierarchical` con bbox y fecha

#### Opci√≥n 2: CLI Script (Python)
- **Para**: Investigadores, notebooks, automatizaci√≥n, CI/CD
- **Ventajas**: Simple, r√°pido, no requiere servidor
- **Uso**: `python analyze_region.py --bbox "..." --date "2025-10-15"`

**Ambas opciones comparten la misma l√≥gica de negocio** (funciones reutilizables).

---

## ‚úÖ Criterios de Aceptaci√≥n

### Funcionalidad Core
- [ ] **API REST Endpoint**: `POST /api/analysis/hierarchical`
  - Par√°metros: bbox, date, threshold_mgrg, export_formats
  - Procesamiento as√≠ncrono con Celery (opcional)
  - Respuesta: analysis_id, status, download_url

- [ ] **CLI Script**: `scripts/analyze_region.py`
  - Argumentos: `--bbox`, `--date`, `--output`, `--threshold`
  - Ejecuci√≥n s√≠ncrona con barra de progreso
  - Output: GeoTIFF, JSON, PNG visualizaciones

- [ ] **Pipeline Completo** (6 pasos):
  1. Descargar Sentinel-2 (bandas HLS)
  2. Extraer embeddings Prithvi
  3. Segmentar con MGRG
  4. Clasificar objetos (zero-shot)
  5. Analizar estr√©s vegetal (solo cultivos)
  6. Generar reporte (JSON + visualizaciones)

### Orquestaci√≥n de M√≥dulos
```python
# Reutilizar componentes existentes:
from src.data.sentinel_downloader import download_sentinel2_hls  # US-003
from src.models.prithvi_wrapper import extract_embeddings        # US-006
from src.algorithms.semantic_region_growing import SemanticRegionGrowing  # US-007
from src.classification.zero_shot_classifier import SemanticClassifier   # US-010
from src.utils.ndvi_calculator import calculate_ndvi             # US-004
```

### Salidas del Sistema
- [ ] **JSON estructurado** con resultados:
  ```json
  {
    "metadata": {"bbox": [...], "date": "2025-10-15", "zone": "Mexicali"},
    "segmentation": {"method": "MGRG", "regions": 156, "coherence": 99.0},
    "classification": [
      {
        "region_id": 5,
        "class": "Vigorous Crop",
        "confidence": 0.87,
        "area_ha": 124.5,
        "stress_level": "low",
        "mean_ndvi": 0.72
      },
      ...
    ],
    "summary": {
      "vigorous_crop_ha": 1245.8,
      "stressed_crop_ha": 523.1,
      "urban_ha": 89.7,
      ...
    }
  }
  ```

- [ ] **GeoTIFF** con capas:
  - Capa 1: Segmentaci√≥n MGRG (region IDs)
  - Capa 2: Clasificaci√≥n sem√°ntica (class IDs)
  - Capa 3: Mapa de estr√©s (stress levels)

- [ ] **Visualizaciones PNG** (300 DPI):
  - Panel 1: RGB original
  - Panel 2: Mapa sem√°ntico clasificado
  - Panel 3: Mapa de estr√©s (solo cultivos)
  - Panel 4: Estad√≠sticas (tabla)

### API REST Espec√≠fico
- [ ] Endpoint documentado en Swagger UI (`/docs`)
- [ ] Validaci√≥n de par√°metros con Pydantic
- [ ] Manejo de errores con c√≥digos HTTP apropiados
- [ ] Rate limiting (10 requests/min por IP)
- [ ] Timeout de 5 minutos m√°ximo

### CLI Script Espec√≠fico
- [ ] Argumentos con `argparse` bien documentados
- [ ] Barra de progreso con `tqdm`
- [ ] Logging a archivo y consola
- [ ] Exit codes informativos (0=success, 1=error)
- [ ] Ejemplo de uso en `--help`

### Testing
- [ ] Tests de integraci√≥n end-to-end (API + CLI)
- [ ] Test con bbox v√°lido (debe completar en <3 min)
- [ ] Test con bbox inv√°lido (debe fallar gracefully)
- [ ] Test de timeout (simular Sentinel-2 lento)

### Cumplimiento AGENTS.md
- [ ] C√≥digo en ingl√©s
- [ ] Documentaci√≥n en espa√±ol
- [ ] Funciones reutilizables en `src/`
- [ ] No c√≥digo duplicado entre API y CLI

---

## üèóÔ∏è Arquitectura de la Soluci√≥n

### Diagrama de Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  SISTEMA DE AN√ÅLISIS JER√ÅRQUICO                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   OPCI√ìN 1: API REST      ‚îÇ        ‚îÇ   OPCI√ìN 2: CLI SCRIPT    ‚îÇ
‚îÇ   (FastAPI + Celery)      ‚îÇ        ‚îÇ   (Python Standalone)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                                     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   CORE PIPELINE LOGIC         ‚îÇ
              ‚îÇ   src/pipeline/hierarchical   ‚îÇ
              ‚îÇ   (Shared by both options)    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚ñº                   ‚ñº                   ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ US-003  ‚îÇ        ‚îÇ US-006  ‚îÇ        ‚îÇ US-007  ‚îÇ
    ‚îÇ Download‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇEmbeddings‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  MGRG   ‚îÇ
    ‚îÇSentinel2‚îÇ        ‚îÇ Prithvi  ‚îÇ        ‚îÇ Segment ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ US-010  ‚îÇ        ‚îÇ  NDVI   ‚îÇ        ‚îÇ OUTPUT  ‚îÇ
    ‚îÇ Classify‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇCalculate‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇGenerator‚îÇ
    ‚îÇ Objects ‚îÇ        ‚îÇ Stress  ‚îÇ        ‚îÇJSON/TIF ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

OUTPUT FINAL:
‚îú‚îÄ‚îÄ analysis_results.json      (Structured data)
‚îú‚îÄ‚îÄ semantic_map.tif           (GeoTIFF 3 layers)
‚îú‚îÄ‚îÄ visualization.png          (300 DPI multi-panel)
‚îî‚îÄ‚îÄ report.html (opcional)     (Interactive report)
```

### Pipeline Detallado

```
STEP 1: INITIALIZATION (0.1s)
‚îú‚îÄ‚îÄ Parse input parameters (bbox, date, thresholds)
‚îú‚îÄ‚îÄ Validate bbox (must be valid WGS84 coordinates)
‚îú‚îÄ‚îÄ Create output directory with timestamp
‚îî‚îÄ‚îÄ Initialize logger

STEP 2: DATA ACQUISITION (5-10s)
‚îú‚îÄ‚îÄ Download Sentinel-2 L2A from Sentinel Hub
‚îÇ   ‚îú‚îÄ‚îÄ Bands: B02, B03, B04, B8A, B11, B12 (HLS format)
‚îÇ   ‚îî‚îÄ‚îÄ Resolution: Resample all to 10m
‚îú‚îÄ‚îÄ Download SCL (Scene Classification Layer) for clouds
‚îî‚îÄ‚îÄ Save raw data to cache (for future re-runs)

STEP 3: EMBEDDING EXTRACTION (10-15s)
‚îú‚îÄ‚îÄ Load Prithvi model (cached after first run)
‚îú‚îÄ‚îÄ Prepare HLS input (6 bands, normalized)
‚îú‚îÄ‚îÄ Forward pass through encoder
‚îú‚îÄ‚îÄ Extract 256D embeddings per pixel
‚îî‚îÄ‚îÄ Save embeddings.npy (H, W, 256)

STEP 4: SEGMENTATION (3-5s)
‚îú‚îÄ‚îÄ Initialize MGRG with threshold=0.95
‚îú‚îÄ‚îÄ Generate grid seeds (20x20 spacing)
‚îú‚îÄ‚îÄ Run region growing with cosine similarity
‚îú‚îÄ‚îÄ Filter regions < 50 pixels
‚îú‚îÄ‚îÄ Save segmentation.npy (H, W) with region IDs
‚îî‚îÄ‚îÄ Calculate metrics: coherence, region count

STEP 5: CLASSIFICATION (1-2s)
‚îú‚îÄ‚îÄ Initialize SemanticClassifier with embeddings + NDVI
‚îú‚îÄ‚îÄ Classify all regions (156 regions ‚Üí 6 classes)
‚îú‚îÄ‚îÄ Generate semantic map (H, W) with class IDs
‚îú‚îÄ‚îÄ Calculate class statistics (area, count, mean NDVI)
‚îî‚îÄ‚îÄ Save classification_results.json

STEP 6: STRESS ANALYSIS (1s)
‚îú‚îÄ‚îÄ Filter only crop regions (class 3 or 4)
‚îú‚îÄ‚îÄ Calculate NDVI statistics per region
‚îú‚îÄ‚îÄ Assign stress level: low/medium/high
‚îú‚îÄ‚îÄ Generate stress map (only crops colored)
‚îî‚îÄ‚îÄ Save stress_analysis.json

STEP 7: OUTPUT GENERATION (2-3s)
‚îú‚îÄ‚îÄ Generate GeoTIFF with 3 layers
‚îÇ   ‚îú‚îÄ‚îÄ Layer 1: Segmentation (region IDs)
‚îÇ   ‚îú‚îÄ‚îÄ Layer 2: Classification (class IDs)
‚îÇ   ‚îî‚îÄ‚îÄ Layer 3: Stress levels
‚îú‚îÄ‚îÄ Generate visualizations (PNG 300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ Panel 1: RGB original
‚îÇ   ‚îú‚îÄ‚îÄ Panel 2: Semantic map (colored)
‚îÇ   ‚îú‚îÄ‚îÄ Panel 3: Stress map (crops only)
‚îÇ   ‚îî‚îÄ‚îÄ Panel 4: Statistics table
‚îú‚îÄ‚îÄ Generate final JSON with all results
‚îî‚îÄ‚îÄ Generate HTML report (optional)

TOTAL TIME: ~25-40 seconds (depending on bbox size and network)
```

---

## üíª Especificaci√≥n T√©cnica

### M√≥dulo Core: `src/pipeline/hierarchical_analysis.py`

```python
"""
End-to-end hierarchical analysis pipeline.

Orchestrates all US components from Sentinel-2 download to stress analysis.
Shared logic between API REST and CLI script.

References:
    - US-003: Sentinel-2 download
    - US-006: Prithvi embeddings
    - US-007: MGRG segmentation
    - US-010: Semantic classification
"""

import numpy as np
from pathlib import Path
from typing import Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import logging
import json
import time

from src.data.sentinel_downloader import download_sentinel2_hls
from src.models.prithvi_wrapper import PrithviModel, extract_embeddings
from src.algorithms.semantic_region_growing import SemanticRegionGrowing
from src.classification.zero_shot_classifier import SemanticClassifier
from src.utils.ndvi_calculator import calculate_ndvi
from src.visualization.report_generator import generate_visualization, generate_geotiff

logger = logging.getLogger(__name__)


@dataclass
class AnalysisConfig:
    """Configuration for hierarchical analysis."""
    bbox: Tuple[float, float, float, float]  # (min_lon, min_lat, max_lon, max_lat)
    date_from: str  # YYYY-MM-DD
    date_to: Optional[str] = None  # If None, use date_from
    mgrg_threshold: float = 0.95
    min_region_size: int = 50
    resolution: float = 10.0  # meters
    output_dir: str = "output/analysis"
    export_formats: list = None  # ["json", "tif", "png", "html"]

    def __post_init__(self):
        if self.export_formats is None:
            self.export_formats = ["json", "tif", "png"]
        if self.date_to is None:
            self.date_to = self.date_from


@dataclass
class AnalysisResult:
    """Complete result of hierarchical analysis."""
    metadata: Dict
    segmentation: Dict
    classification: list
    stress_analysis: Dict
    summary: Dict
    output_files: Dict


class HierarchicalAnalysisPipeline:
    """
    Complete pipeline for hierarchical land cover and stress analysis.

    Parameters
    ----------
    config : AnalysisConfig
        Configuration object with all parameters

    Examples
    --------
    >>> config = AnalysisConfig(
    ...     bbox=(-115.35, 32.45, -115.25, 32.55),
    ...     date_from="2025-10-15"
    ... )
    >>> pipeline = HierarchicalAnalysisPipeline(config)
    >>> result = pipeline.run()
    >>> print(result.summary['vigorous_crop_ha'])
    1245.8
    """

    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize timer
        self.start_time = time.time()
        self.step_times = {}

        logger.info(f"HierarchicalAnalysisPipeline initialized")
        logger.info(f"BBox: {config.bbox}")
        logger.info(f"Date: {config.date_from}")
        logger.info(f"Output: {self.output_dir}")

    def _time_step(self, step_name: str):
        """Record time for a pipeline step."""
        elapsed = time.time() - self.start_time
        self.step_times[step_name] = elapsed
        logger.info(f"[{step_name}] Completed in {elapsed:.2f}s")

    def run(self) -> AnalysisResult:
        """
        Execute complete analysis pipeline.

        Returns
        -------
        AnalysisResult
            Complete results with all outputs

        Raises
        ------
        ValueError
            If bbox or date is invalid
        RuntimeError
            If any pipeline step fails
        """
        logger.info("=" * 60)
        logger.info("Starting Hierarchical Analysis Pipeline")
        logger.info("=" * 60)

        try:
            # Step 1: Data Acquisition
            logger.info("\n[STEP 1/6] Data Acquisition")
            hls_data, metadata = self._download_sentinel2()
            self._time_step("download")

            # Step 2: Embedding Extraction
            logger.info("\n[STEP 2/6] Extracting Prithvi Embeddings")
            embeddings = self._extract_embeddings(hls_data)
            self._time_step("embeddings")

            # Step 3: Segmentation
            logger.info("\n[STEP 3/6] MGRG Segmentation")
            segmentation, seg_metrics = self._segment(embeddings)
            self._time_step("segmentation")

            # Step 4: NDVI Calculation
            logger.info("\n[STEP 4/6] Calculating NDVI")
            ndvi = self._calculate_ndvi(hls_data)
            self._time_step("ndvi")

            # Step 5: Classification
            logger.info("\n[STEP 5/6] Semantic Classification")
            classifications, semantic_map = self._classify(
                embeddings, ndvi, segmentation
            )
            self._time_step("classification")

            # Step 6: Stress Analysis
            logger.info("\n[STEP 6/6] Stress Analysis")
            stress_results = self._analyze_stress(classifications, ndvi)
            self._time_step("stress")

            # Generate outputs
            logger.info("\n[OUTPUT] Generating Files")
            output_files = self._generate_outputs(
                hls_data, segmentation, semantic_map,
                classifications, stress_results
            )
            self._time_step("output")

            # Compile results
            result = self._compile_results(
                metadata, seg_metrics, classifications,
                stress_results, output_files
            )

            total_time = time.time() - self.start_time
            logger.info(f"\n{'=' * 60}")
            logger.info(f"Pipeline completed in {total_time:.2f}s")
            logger.info(f"{'=' * 60}")

            return result

        except Exception as e:
            logger.error(f"Pipeline failed: {e}", exc_info=True)
            raise RuntimeError(f"Analysis pipeline failed: {e}")

    def _download_sentinel2(self) -> Tuple[np.ndarray, Dict]:
        """Download Sentinel-2 HLS data."""
        hls_data, metadata = download_sentinel2_hls(
            bbox=self.config.bbox,
            date_from=self.config.date_from,
            date_to=self.config.date_to,
            bands=['B02', 'B03', 'B04', 'B8A', 'B11', 'B12'],
            resolution=self.config.resolution
        )

        logger.info(f"Downloaded HLS data: {hls_data.shape}")
        return hls_data, metadata

    def _extract_embeddings(self, hls_data: np.ndarray) -> np.ndarray:
        """Extract Prithvi embeddings."""
        model = PrithviModel()  # Singleton, cached
        embeddings = extract_embeddings(model, hls_data)

        # Save embeddings (useful for debugging)
        emb_path = self.output_dir / "embeddings.npy"
        np.save(emb_path, embeddings)

        logger.info(f"Embeddings extracted: {embeddings.shape}")
        return embeddings

    def _segment(self, embeddings: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Run MGRG segmentation."""
        mgrg = SemanticRegionGrowing(
            threshold=self.config.mgrg_threshold,
            min_region_size=self.config.min_region_size,
            use_smart_seeds=False  # Grid is superior (US-007)
        )

        segmentation = mgrg.segment(embeddings)

        # Calculate metrics
        from src.utils.comparison_metrics import (
            calculate_spatial_coherence,
            count_regions
        )

        metrics = {
            'method': 'MGRG',
            'threshold': self.config.mgrg_threshold,
            'regions': count_regions(segmentation),
            'coherence': calculate_spatial_coherence(segmentation),
        }

        # Save segmentation
        seg_path = self.output_dir / "segmentation.npy"
        np.save(seg_path, segmentation)

        logger.info(f"Segmentation: {metrics['regions']} regions, "
                   f"{metrics['coherence']:.1f}% coherence")

        return segmentation, metrics

    def _calculate_ndvi(self, hls_data: np.ndarray) -> np.ndarray:
        """Calculate NDVI from HLS bands."""
        # HLS bands: [B02, B03, B04, B8A, B11, B12]
        # NDVI = (NIR - Red) / (NIR + Red)
        # NIR = B8A (index 3), Red = B04 (index 2)
        nir = hls_data[:, :, 3]
        red = hls_data[:, :, 2]

        ndvi = calculate_ndvi(nir, red)

        # Save NDVI
        ndvi_path = self.output_dir / "ndvi.npy"
        np.save(ndvi_path, ndvi)

        logger.info(f"NDVI calculated: mean={ndvi.mean():.3f}, "
                   f"std={ndvi.std():.3f}")

        return ndvi

    def _classify(
        self,
        embeddings: np.ndarray,
        ndvi: np.ndarray,
        segmentation: np.ndarray
    ) -> Tuple[Dict, np.ndarray]:
        """Classify all regions."""
        classifier = SemanticClassifier(
            embeddings, ndvi, resolution=self.config.resolution
        )

        classifications = classifier.classify_all_regions(segmentation)
        semantic_map = classifier.generate_semantic_map(
            segmentation, classifications
        )

        # Save semantic map
        sem_path = self.output_dir / "semantic_map.npy"
        np.save(sem_path, semantic_map)

        logger.info(f"Classified {len(classifications)} regions")

        return classifications, semantic_map

    def _analyze_stress(
        self,
        classifications: Dict,
        ndvi: np.ndarray
    ) -> Dict:
        """Analyze stress for crop regions."""
        stress_results = {
            'low': {'count': 0, 'area_ha': 0.0, 'regions': []},
            'medium': {'count': 0, 'area_ha': 0.0, 'regions': []},
            'high': {'count': 0, 'area_ha': 0.0, 'regions': []},
        }

        for region_id, result in classifications.items():
            # Only analyze crops (class 3 or 4)
            if result.class_id not in [3, 4]:
                continue

            mean_ndvi = result.mean_ndvi

            # Assign stress level
            if mean_ndvi >= 0.55:
                level = 'low'
            elif mean_ndvi >= 0.40:
                level = 'medium'
            else:
                level = 'high'

            stress_results[level]['count'] += 1
            stress_results[level]['area_ha'] += result.area_hectares
            stress_results[level]['regions'].append({
                'region_id': region_id,
                'ndvi': mean_ndvi,
                'area_ha': result.area_hectares
            })

        logger.info(f"Stress analysis: "
                   f"Low={stress_results['low']['count']}, "
                   f"Medium={stress_results['medium']['count']}, "
                   f"High={stress_results['high']['count']}")

        return stress_results

    def _generate_outputs(
        self,
        hls_data: np.ndarray,
        segmentation: np.ndarray,
        semantic_map: np.ndarray,
        classifications: Dict,
        stress_results: Dict
    ) -> Dict[str, str]:
        """Generate all output files."""
        output_files = {}

        # 1. JSON (always generated)
        if "json" in self.config.export_formats:
            json_path = self.output_dir / "analysis_results.json"
            self._save_json(classifications, stress_results, json_path)
            output_files['json'] = str(json_path)
            logger.info(f"Generated JSON: {json_path}")

        # 2. GeoTIFF (optional)
        if "tif" in self.config.export_formats:
            tif_path = self.output_dir / "semantic_map.tif"
            generate_geotiff(
                segmentation, semantic_map,
                self.config.bbox, tif_path
            )
            output_files['tif'] = str(tif_path)
            logger.info(f"Generated GeoTIFF: {tif_path}")

        # 3. PNG Visualization (optional)
        if "png" in self.config.export_formats:
            png_path = self.output_dir / "visualization.png"
            rgb = hls_data[:, :, [2, 1, 0]]  # B04, B03, B02 ‚Üí RGB
            generate_visualization(
                rgb, semantic_map, classifications,
                stress_results, png_path
            )
            output_files['png'] = str(png_path)
            logger.info(f"Generated PNG: {png_path}")

        # 4. HTML Report (optional)
        if "html" in self.config.export_formats:
            html_path = self.output_dir / "report.html"
            # TODO: Implement HTML report generator
            output_files['html'] = str(html_path)

        return output_files

    def _save_json(
        self,
        classifications: Dict,
        stress_results: Dict,
        output_path: Path
    ):
        """Save results as structured JSON."""
        # Convert classifications to list of dicts
        classification_list = [
            {
                'region_id': int(region_id),
                'class': result.class_name,
                'class_id': result.class_id,
                'confidence': result.confidence,
                'area_ha': result.area_hectares,
                'mean_ndvi': result.mean_ndvi,
                'std_ndvi': result.std_ndvi,
            }
            for region_id, result in classifications.items()
        ]

        # Calculate summary
        from collections import defaultdict
        summary = defaultdict(float)
        for result in classifications.values():
            summary[f"{result.class_name.lower().replace(' ', '_')}_ha"] += \
                result.area_hectares

        # Compile final JSON
        output_data = {
            'metadata': {
                'bbox': list(self.config.bbox),
                'date_from': self.config.date_from,
                'date_to': self.config.date_to,
                'resolution': self.config.resolution,
                'mgrg_threshold': self.config.mgrg_threshold,
            },
            'segmentation': {
                'method': 'MGRG',
                'regions': len(classifications),
            },
            'classification': classification_list,
            'stress_analysis': stress_results,
            'summary': dict(summary),
            'processing_time': self.step_times,
        }

        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2)

    def _compile_results(
        self,
        metadata: Dict,
        seg_metrics: Dict,
        classifications: Dict,
        stress_results: Dict,
        output_files: Dict
    ) -> AnalysisResult:
        """Compile all results into AnalysisResult object."""
        # Convert classifications to list
        classification_list = [
            asdict(result) for result in classifications.values()
        ]

        # Calculate summary
        from collections import defaultdict
        summary = defaultdict(float)
        for result in classifications.values():
            key = f"{result.class_name.lower().replace(' ', '_')}_ha"
            summary[key] += result.area_hectares

        return AnalysisResult(
            metadata=metadata,
            segmentation=seg_metrics,
            classification=classification_list,
            stress_analysis=stress_results,
            summary=dict(summary),
            output_files=output_files
        )
```

### API REST: `backend/app/api/routes/hierarchical.py`

```python
"""
FastAPI endpoint for hierarchical analysis.

Usage:
    POST /api/analysis/hierarchical
    Body: {
        "bbox": [-115.35, 32.45, -115.25, 32.55],
        "date_from": "2025-10-15",
        "mgrg_threshold": 0.95,
        "export_formats": ["json", "tif", "png"]
    }
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Tuple
import uuid
from pathlib import Path

from src.pipeline.hierarchical_analysis import (
    HierarchicalAnalysisPipeline,
    AnalysisConfig,
    AnalysisResult
)

router = APIRouter(prefix="/api/analysis", tags=["analysis"])


class HierarchicalAnalysisRequest(BaseModel):
    """Request schema for hierarchical analysis."""
    bbox: Tuple[float, float, float, float] = Field(
        ...,
        description="Bounding box (min_lon, min_lat, max_lon, max_lat)"
    )
    date_from: str = Field(..., description="Start date YYYY-MM-DD")
    date_to: Optional[str] = Field(None, description="End date (optional)")
    mgrg_threshold: float = Field(0.95, ge=0.7, le=0.99)
    min_region_size: int = Field(50, ge=10, le=500)
    export_formats: List[str] = Field(["json", "tif", "png"])

    @validator('bbox')
    def validate_bbox(cls, v):
        if not (-180 <= v[0] < v[2] <= 180 and -90 <= v[1] < v[3] <= 90):
            raise ValueError("Invalid bbox coordinates")
        return v

    @validator('date_from', 'date_to')
    def validate_date(cls, v):
        if v is None:
            return v
        # Simple validation (can be improved)
        if not (len(v) == 10 and v[4] == '-' and v[7] == '-'):
            raise ValueError("Date must be YYYY-MM-DD format")
        return v


class HierarchicalAnalysisResponse(BaseModel):
    """Response schema for hierarchical analysis."""
    analysis_id: str
    status: str  # "processing" | "completed" | "failed"
    message: str
    output_files: Optional[dict] = None


# In-memory storage (replace with Redis/DB in production)
analysis_status = {}


@router.post("/hierarchical", response_model=HierarchicalAnalysisResponse)
async def run_hierarchical_analysis(
    request: HierarchicalAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    Run complete hierarchical analysis pipeline.

    Returns immediately with analysis_id.
    Processing runs in background (async).
    """
    # Generate unique ID
    analysis_id = str(uuid.uuid4())

    # Create config
    config = AnalysisConfig(
        bbox=request.bbox,
        date_from=request.date_from,
        date_to=request.date_to,
        mgrg_threshold=request.mgrg_threshold,
        min_region_size=request.min_region_size,
        export_formats=request.export_formats,
        output_dir=f"output/api/{analysis_id}"
    )

    # Initialize status
    analysis_status[analysis_id] = {
        'status': 'processing',
        'message': 'Analysis started',
    }

    # Run pipeline in background
    background_tasks.add_task(
        run_pipeline_background,
        analysis_id,
        config
    )

    return HierarchicalAnalysisResponse(
        analysis_id=analysis_id,
        status="processing",
        message="Analysis started. Use GET /analysis/{id}/status to check progress."
    )


async def run_pipeline_background(analysis_id: str, config: AnalysisConfig):
    """Run pipeline in background task."""
    try:
        pipeline = HierarchicalAnalysisPipeline(config)
        result = pipeline.run()

        # Update status
        analysis_status[analysis_id] = {
            'status': 'completed',
            'message': 'Analysis completed successfully',
            'output_files': result.output_files,
            'summary': result.summary,
        }

    except Exception as e:
        analysis_status[analysis_id] = {
            'status': 'failed',
            'message': f"Analysis failed: {str(e)}",
        }


@router.get("/hierarchical/{analysis_id}/status")
async def get_analysis_status(analysis_id: str):
    """Get status of analysis."""
    if analysis_id not in analysis_status:
        raise HTTPException(status_code=404, detail="Analysis not found")

    return analysis_status[analysis_id]


@router.get("/hierarchical/{analysis_id}/download/{file_type}")
async def download_analysis_result(analysis_id: str, file_type: str):
    """Download result file (json, tif, png)."""
    if analysis_id not in analysis_status:
        raise HTTPException(status_code=404, detail="Analysis not found")

    status = analysis_status[analysis_id]
    if status['status'] != 'completed':
        raise HTTPException(status_code=400, detail="Analysis not completed")

    # Get file path
    output_files = status.get('output_files', {})
    file_path = output_files.get(file_type)

    if not file_path or not Path(file_path).exists():
        raise HTTPException(status_code=404, detail=f"File type {file_type} not found")

    # Determine media type
    media_types = {
        'json': 'application/json',
        'tif': 'image/tiff',
        'png': 'image/png',
        'html': 'text/html',
    }

    return FileResponse(
        file_path,
        media_type=media_types.get(file_type, 'application/octet-stream'),
        filename=Path(file_path).name
    )
```

### CLI Script: `scripts/analyze_region.py`

```python
#!/usr/bin/env python3
"""
Command-line interface for hierarchical analysis.

Usage:
    python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15"
    python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15" --output "output/mexicali"
    python analyze_region.py --help

Examples:
    # Basic analysis
    python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15"

    # Custom threshold
    python analyze_region.py --bbox "..." --date "..." --threshold 0.90

    # Export specific formats
    python analyze_region.py --bbox "..." --date "..." --formats json,tif

    # Verbose logging
    python analyze_region.py --bbox "..." --date "..." --verbose
"""

import argparse
import sys
import logging
from pathlib import Path
from typing import Tuple

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.pipeline.hierarchical_analysis import (
    HierarchicalAnalysisPipeline,
    AnalysisConfig
)


def parse_bbox(bbox_str: str) -> Tuple[float, float, float, float]:
    """Parse bbox string to tuple."""
    try:
        coords = [float(x) for x in bbox_str.split(',')]
        if len(coords) != 4:
            raise ValueError("BBox must have 4 coordinates")
        return tuple(coords)
    except Exception as e:
        raise ValueError(f"Invalid bbox format: {e}")


def setup_logging(verbose: bool = False):
    """Setup logging configuration."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('analysis.log')
        ]
    )


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Hierarchical Analysis CLI - Region Growing System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze Mexicali region
  python analyze_region.py --bbox "32.45,-115.35,32.55,-115.25" --date "2025-10-15"

  # Custom output directory
  python analyze_region.py --bbox "..." --date "..." --output "results/mexicali_20251015"

  # Export only JSON and PNG
  python analyze_region.py --bbox "..." --date "..." --formats json,png

  # Adjust MGRG threshold
  python analyze_region.py --bbox "..." --date "..." --threshold 0.90

For more information: https://github.com/your-repo/region-growing
        """
    )

    # Required arguments
    parser.add_argument(
        '--bbox',
        type=str,
        required=True,
        help='Bounding box as "min_lat,min_lon,max_lat,max_lon" (WGS84)'
    )
    parser.add_argument(
        '--date',
        type=str,
        required=True,
        help='Analysis date as YYYY-MM-DD (e.g., 2025-10-15)'
    )

    # Optional arguments
    parser.add_argument(
        '--output',
        type=str,
        default='output/cli',
        help='Output directory (default: output/cli)'
    )
    parser.add_argument(
        '--threshold',
        type=float,
        default=0.95,
        help='MGRG similarity threshold 0.7-0.99 (default: 0.95)'
    )
    parser.add_argument(
        '--min-size',
        type=int,
        default=50,
        help='Minimum region size in pixels (default: 50)'
    )
    parser.add_argument(
        '--formats',
        type=str,
        default='json,tif,png',
        help='Export formats comma-separated: json,tif,png,html (default: json,tif,png)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )

    args = parser.parse_args()

    # Setup logging
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)

    try:
        # Parse inputs
        bbox = parse_bbox(args.bbox)
        export_formats = args.formats.split(',')

        # Create config
        config = AnalysisConfig(
            bbox=bbox,
            date_from=args.date,
            mgrg_threshold=args.threshold,
            min_region_size=args.min_size,
            output_dir=args.output,
            export_formats=export_formats
        )

        logger.info("="*60)
        logger.info("Hierarchical Analysis CLI")
        logger.info("="*60)
        logger.info(f"BBox: {bbox}")
        logger.info(f"Date: {args.date}")
        logger.info(f"Output: {args.output}")
        logger.info(f"Threshold: {args.threshold}")
        logger.info("="*60)

        # Run pipeline
        pipeline = HierarchicalAnalysisPipeline(config)
        result = pipeline.run()

        # Print results
        print("\n" + "="*60)
        print("ANALYSIS COMPLETE")
        print("="*60)
        print(f"\nOutput files:")
        for file_type, file_path in result.output_files.items():
            print(f"  {file_type.upper()}: {file_path}")

        print(f"\nSummary:")
        for key, value in result.summary.items():
            print(f"  {key}: {value:.1f} ha")

        print("\n" + "="*60)

        return 0  # Success

    except KeyboardInterrupt:
        logger.warning("Analysis interrupted by user")
        return 130  # SIGINT

    except Exception as e:
        logger.error(f"Analysis failed: {e}", exc_info=True)
        print(f"\nERROR: {e}", file=sys.stderr)
        print("Use --verbose for detailed error information", file=sys.stderr)
        return 1  # Error


if __name__ == "__main__":
    sys.exit(main())
```

---

## üìä Plan de Desarrollo

### Fase 1: Core Pipeline (2 horas)
1. **Crear `src/pipeline/hierarchical_analysis.py`** (1.5h)
   - Clase `HierarchicalAnalysisPipeline`
   - Orquestaci√≥n de 6 pasos
   - Manejo de errores
   - Logging detallado

2. **Tests Unitarios Pipeline** (30 min)
   - Mock de componentes externos
   - Test happy path
   - Test error handling

### Fase 2: API REST (2 horas)
1. **Endpoint POST /api/analysis/hierarchical** (1h)
   - Schema Pydantic
   - Background task con FastAPI
   - Status storage

2. **Endpoints auxiliares** (1h)
   - GET /status
   - GET /download
   - Error handling
   - Rate limiting

### Fase 3: CLI Script (1.5 horas)
1. **Script `scripts/analyze_region.py`** (1h)
   - Argumentos con argparse
   - Llamada a pipeline
   - Progress bar con tqdm
   - Exit codes

2. **Documentaci√≥n y Ejemplos** (30 min)
   - Help text detallado
   - README con ejemplos
   - Casos de uso comunes

### Fase 4: Testing e Integraci√≥n (30 min)
1. **Tests End-to-End** (30 min)
   - Test CLI con bbox v√°lido
   - Test API con mock request
   - Verificar outputs generados

---

## üß™ Testing

```python
# tests/integration/test_hierarchical_pipeline.py

def test_cli_script_success():
    """CLI should complete successfully with valid inputs"""
    result = subprocess.run([
        'python', 'scripts/analyze_region.py',
        '--bbox', '32.45,-115.35,32.55,-115.25',
        '--date', '2025-10-15',
        '--output', 'tmp/test_output'
    ], capture_output=True)

    assert result.returncode == 0
    assert Path('tmp/test_output/analysis_results.json').exists()

def test_api_endpoint_success(client):
    """API should return analysis_id and process in background"""
    response = client.post('/api/analysis/hierarchical', json={
        'bbox': [32.45, -115.35, 32.55, -115.25],
        'date_from': '2025-10-15'
    })

    assert response.status_code == 200
    data = response.json()
    assert 'analysis_id' in data
    assert data['status'] == 'processing'
```

---

## ‚ö†Ô∏è Riesgos y Mitigaciones

### Riesgo 1: Timeout en descarga Sentinel-2
**Mitigaci√≥n**: Timeout de 5 min, retry con exponential backoff

### Riesgo 2: Memoria insuficiente para im√°genes grandes
**Mitigaci√≥n**: Limitar bbox m√°ximo, procesar por tiles

### Riesgo 3: API sobrecargada (muchos requests)
**Mitigaci√≥n**: Rate limiting 10 req/min, queue con Celery

---

## ‚úÖ Definici√≥n de Completado

- [x] `src/pipeline/hierarchical_analysis.py` implementado
- [x] API REST endpoint funcional
- [x] CLI script funcional
- [x] Tests end-to-end
- [x] Documentaci√≥n completa (README + docstrings)
- [x] Ejemplos de uso
- [x] C√≥digo siguiendo AGENTS.md

---

**Autor**: Carlos Bocanegra (API), Arthur Zizumbo (Pipeline + CLI)
**Fecha**: 11 de Noviembre de 2025
**Versi√≥n**: 1.0 (Final)
